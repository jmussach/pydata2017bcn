{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np #scientific library\n",
    "import scipy #scientific library\n",
    "import scipy.io.wavfile\n",
    "import sys, os #file reading, directory parsing routines\n",
    "import matplotlib #to plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio file reading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use scipy.io to read and write audio files, without any external library. \n",
    "In this example we read a stereo file of shape (132299, 2), where the first dimension is the time and the second represents the left and right channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path_to_irmas = './IRMAS-Sample/'\n",
    "\n",
    "#read audio file\n",
    "audio_file = '001__[vio][nod][cou_fol]2194__1.wav'\n",
    "sampleRate, audioObj = scipy.io.wavfile.read(os.path.join(path_to_irmas,'Training','vio',audio_file)) \n",
    "\n",
    "print audioObj.shape\n",
    "plt.plot(audioObj)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in the plot above, the array is not normalized.\n",
    "\n",
    "We included a function to read an audio file in util.py, which does this normalization step and also returns the bitrate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import util\n",
    "\n",
    "audio, sampleRate, bitrate = util.readAudioScipy(os.path.join(path_to_irmas,'Training','vio',audio_file)) \n",
    "\n",
    "plt.plot(audio.sum(axis=1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a single channel audio is desired, we can sum the two channels: audio.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract features from audio which can be used for training the neural network.\n",
    "\n",
    "In transform.py you can find a function which computes the Short-term Fourier transform (STFT) of a single-channel audio. The STFT spectrogram is computed for overlapping windows comprising nfft=1024 samples (0.011s). The overlap between these windows is hopsize=512 samples (0.022s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transform\n",
    "\n",
    "spectrogram = transform.stft_norm(audio.sum(axis=1), window=np.hanning(1024), hopsize=512, nfft=1024, fs=float(sampleRate))\n",
    "\n",
    "spectrogram.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns 261 vectors for each overlapping frame. Each vector has a feature size of 513.\n",
    "\n",
    "Usually, we discard the phase, and we obtain the magnitude spectrogram (real numbers).\n",
    "\n",
    "We also need to normalize by the size of the window. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print spectrogram[0,0]\n",
    "mag = np.abs(spectrogram)\n",
    "print mag[0,0]\n",
    "\n",
    "mag = mag  / np.sqrt(1024) #normalization\n",
    "mag.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this spectrogram as an image with matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print plt.style.available\n",
    "plt.style.use('classic')\n",
    "plt.imshow(np.log10(1+100*mag).T,interpolation='none', origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can easily distinguish evenly-spaces lines which correspond to harmonic partials of a played instrument note. Take a look at how different notes appear in the image and what is the relation between the harmonic partials. We will try to learn this information from spectrograms to discriminate between instruments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many more time-frequency representations than the STFT, some of them relying on the computation of STFT, as the mel spectrogram, widely used in speech processing. \n",
    "\n",
    "To obtain a mel spectrogram we need to obtain a set of mel filters. We can use librosa for that and we included the mel function from this library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mel_basis = transform.mel(sampleRate, n_fft=1024, n_mels=96, fmin=0, fmax=float(sampleRate)/2, htk=False,norm=1)\n",
    "mel_basis.shape      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we multiply the STFT magnitude spectrogram with the mel filters to obtain the mel spectrogram.\n",
    "\n",
    "Be careful with the dimensions. Some library consider first dimension time and frequency the second, while this might change for other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "melspec = np.dot(mel_basis, mag.T)\n",
    "melspec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the mel spectrogram. \n",
    "As you can see we reduced the number of features from 513 to 96 and we put more emphasis on the lower frequencies which are better discriminated by the human perception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.log10(1+100*melspec),interpolation='none', origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we save the mel spectrogram to a binary file. We also need to save the shape to a text file, so we can reshape the numpy array when we load the file.\n",
    "This way of saving data to disk is faster than using pickle or npz files. \n",
    "\n",
    "The functions saveTensor and loadTensor can be found in util.py .  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_dir_train = os.path.join(path_to_irmas,'features','Training')\n",
    "if not os.path.exists(feature_dir_train):\n",
    "    os.makedirs(feature_dir_train)\n",
    "\n",
    "#save the features to file\n",
    "melspec.tofile(os.path.join(feature_dir_train,audio_file.replace('.wav','.data')))\n",
    "#load the features from file\n",
    "melspecin = np.fromfile(os.path.join(feature_dir_train,audio_file.replace('.wav','.data')))\n",
    "print 'input spectrogram shape '+str(melspecin.shape)\n",
    "#we need to save the shape\n",
    "melspecin = melspecin.reshape(melspec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In transform.py we provide a class that does this feature computation for you: transformMEL. This class has an associated method compute_transform which can compute the mel spectrogram and save it to a '.data' file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transform import transformMEL\n",
    "tr = transformMEL(bins=96, frameSize=1024, hopSize=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the training features for all the audio files in the training dataset. \n",
    "We go through the instrument list and we read all the wave files from the corresponding directories. Then, we compute the features using compute_transform. \n",
    "We also need to save the labels for each sound example.\n",
    "\n",
    "The suffix in compute_transform is used to discriminate between various features. In this case we use '_mel_' for mel spectrogram and '_label_' for the label associated with the instrument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d=os.path.join(path_to_irmas,'Training')\n",
    "instruments = sorted(filter(lambda x: os.path.isdir(os.path.join(d, x)), os.listdir(d)))\n",
    "\n",
    "for count,inst in enumerate(instruments):\n",
    "    for f in os.listdir(os.path.join(d,inst)):\n",
    "        if os.path.isfile(os.path.join(d,inst, f)) and f.endswith('.wav'):\n",
    "            audio, sampleRate, bitrate = util.readAudioScipy(os.path.join(d,inst,f)) \n",
    "            tr.compute_transform(audio.sum(axis=1),out_path=os.path.join(feature_dir_train,f.replace('.wav','.data')),suffix='_mel_',sampleRate=sampleRate)\n",
    "            util.saveTensor(np.array([count],dtype=float),out_path=os.path.join(feature_dir_train,f.replace('.wav','.data')),suffix='_label_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that saveTensor and loadTensor work with float numbers, thus we have to convert our labels from int to float. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch generation is a pre-training stage in which we read '.data' files from disk and we group them into batches. Other steps such as normalization, PCA whitening can be implemented at this stage. We also need to shuffle all examples as presenting them grouped by instruments can create bias in training. \n",
    "\n",
    "Training happens sequentially batch by batch until there is no data left. Then we shuffle the data again and we repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a list of the .data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of the '.data' files we computed previously. We use suffix_in and suffix_out to filter the feature files. In this case, we have mel spectrogram as input ('_mel') and instrument labels as output ('_label_')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#build a list with all the .data files in the training dataset \n",
    "feature_dir_train = os.path.join(path_to_irmas,'features','Training')\n",
    "suffix_in='_mel_'\n",
    "suffix_out='_label_'\n",
    "file_list = [f for f in os.listdir(feature_dir_train) \n",
    "            if f.endswith(suffix_in+'.data') and \n",
    "            os.path.isfile(os.path.join(feature_dir_train,f.replace(suffix_in,suffix_out))) ]\n",
    "print 'training file list: \\n'+str(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the first spectrogram to confirm that the data is correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's load the first file\n",
    "melspec = util.loadTensor(out_path=os.path.join(feature_dir_train,file_list[0]))\n",
    "plt.imshow(np.log10(1+100*melspec.T),interpolation='none', origin='lower')\n",
    "plt.show()\n",
    "print 'input spectrogram shape '+str(melspec.shape)\n",
    "label = util.loadTensor(out_path=os.path.join(feature_dir_train,file_list[0].replace('mel','label')))\n",
    "print 'label of the instrument '+str(label)+', representing '+instruments[int(label)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocating memory for the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks (CNN) usually work with images of a specific size. Usually if we change the resolution or the shape of the image, we need to train a new model. Therefore, we need to cut or spectrogram into time_context (x-axis) blocks. Similarly, if we increase the resolution of the spectrogram, we have more features on the y-axis and we need to re-train the network.\n",
    "\n",
    "In this case, the CNN models a time_context of 128 frames (128*0.011s=1.4s). We split each example in blocks of this size. We can create more data by overlapping these blocks. In this case we use an step of 50 time frames. In total, we generate 3 examples (instances or training data points) from the previously loaded '.data' file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#parameters to generate blocks of data from spectrograms\n",
    "time_context=128 #time context modeled by the network\n",
    "step = 50 #step to generate more blocks\n",
    "\n",
    "noinstances = np.maximum(1,int(np.ceil((float(melspec.shape[0])-time_context)/ step)))\n",
    "\n",
    "print \"number of instances: \"+str(noinstances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function that computes the number of examples we can generate for a given file. \n",
    "\n",
    "We can call it for all the features '.data' files to see how many training examples we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getNumInstances(infile,time_context=100,step=25):\n",
    "    \"\"\"\n",
    "    For a single .data file computes the number of examples of size \\\"time_context\\\" that can be created\n",
    "    \"\"\"\n",
    "    shape = util.get_shape(os.path.join(infile.replace('.data','.shape')))\n",
    "    length_file = float(shape[0])\n",
    "    return np.maximum(1,int(np.ceil((length_file-time_context)/ step))) \n",
    "\n",
    "noinstances=getNumInstances(infile=os.path.join(feature_dir_train,file_list[0]),time_context=time_context,step=step)\n",
    "print \"number of instances: \"+str(noinstances)\n",
    "\n",
    "total_noinstances = np.cumsum(np.array([0]+[ getNumInstances(os.path.join(feature_dir_train,infile),time_context=time_context,step=step) for infile in file_list], dtype=int))\n",
    "print \"cumulative sum of number of instances per .data file: \"+str(total_noinstances)\n",
    "total_points = total_noinstances[-1]\n",
    "print \"total number of instances \"+str(total_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can determine the feature_size for the input features. We can read this from the '.shape' file which is associated with a '.data' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatureSize(infile):\n",
    "    \"\"\"\n",
    "    For a single .data file return the number of feature, e.g. number of spectrogram bins\n",
    "    \"\"\"\n",
    "    shape = util.get_shape(os.path.join(infile.replace('.data','.shape')))\n",
    "    return shape[1]\n",
    "\n",
    "feature_size = getFeatureSize(infile=os.path.join(feature_dir_train,file_list[0]))\n",
    "print \"feature size \"+str(feature_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this information to allocate data for the features and the labels. If you use a GPU like TitanX, allocate with np.float32 as it saves memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "floatX=np.float32\n",
    "####features\n",
    "features=np.zeros((total_points,time_context,feature_size),dtype=floatX)\n",
    "####labels\n",
    "labels = np.zeros((total_points,1),dtype=floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's slice the first .data file with index id=0 and store it in the features array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![slicing the spectrogram](img/slice.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id = 0\n",
    "spec = util.loadTensor(out_path=os.path.join(feature_dir_train,file_list[id]))\n",
    "print 'shape of spectrogram '+str(spec.shape)\n",
    "lab = util.loadTensor(out_path=os.path.join(feature_dir_train,file_list[id].replace(suffix_in,suffix_out)))\n",
    "#we need to put the features in the self.features array starting at this index\n",
    "idx_start = total_noinstances[id]\n",
    "#and we stop at this index in self.feature\n",
    "idx_end = total_noinstances[id+1]\n",
    "\n",
    "#copy each block of size (time_contex,feature_size) in the self.features\n",
    "idx = 0 #starting index of each block\n",
    "start = 0 #starting point for each block in frames\n",
    "fig, ax = plt.subplots(nrows=1,ncols=idx_end-idx_start)\n",
    "while idx<(idx_end-idx_start):\n",
    "    print 'segment '+str(idx) + ' from '+str(start)+ ' to '+ str(start+time_context)\n",
    "    features[idx_start+idx] = spec[start:start+time_context]\n",
    "    plt.subplot(idx_end-idx_start,3,idx+1) \n",
    "    plt.imshow(np.log10(1+100*features[idx_start+idx].T),interpolation='none', origin='lower')\n",
    "    start = start + step\n",
    "    idx = idx + 1\n",
    "labels[idx_start:idx_end] = lab[0]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We populate this vector with the examples we generate from each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetchFile(self,id):\n",
    "    #load the data files\n",
    "    spec = util.loadTensor(out_path=os.path.join(self.feature_dir,self.file_list[id]))\n",
    "    lab = util.loadTensor(out_path=os.path.join(self.feature_dir,self.file_list[id].replace(self.suffix_in,self.suffix_out)))\n",
    "    #we need to put the features in the self.features array starting at this index\n",
    "    idx_start = self.total_noinstances[id]\n",
    "    #and we stop at this index in self.feature\n",
    "    idx_end = self.total_noinstances[id+1]\n",
    "\n",
    "    #copy each block of size (time_contex,feature_size) in the self.features\n",
    "    idx=0 #starting index of each block\n",
    "    start = 0 #starting point for each block in frames\n",
    "    while idx<(idx_end-idx_start):\n",
    "        self.features[idx_start+idx] = spec[start:start+self.time_context]\n",
    "        start = start + self.step\n",
    "        idx = idx + 1\n",
    "    self.labels[idx_start:idx_end] = lab[0]\n",
    "    spec = None\n",
    "    lab = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapping everything together, we can put all these routines in a class, which reads the data from the disk and returns batches for the training. \n",
    "\n",
    "If we have a total number of 36 training examples we keep 20% for validation(train_percent=0.8), then we are left with 29 training examples. With a batch_size=6, this means that we will have 4 batches of 6 examples each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dataset\n",
    "from dataset import MyDataset\n",
    "\n",
    "db=MyDataset(feature_dir=feature_dir_train, batch_size=6, time_context=128, step=50, \n",
    "             suffix_in='_mel_',suffix_out='_label_',floatX=np.float32,train_percent=0.8)\n",
    "print \"total number of instances: \"+str(db.total_points)\n",
    "print \"batch_size: \"+str(db.batch_size)\n",
    "print \"iteration size: \"+str(db.iteration_size)\n",
    "print \"feature shape: \"+str(db.features.shape)\n",
    "print \"labels shape: \"+str(db.labels.shape)\n",
    "print \"feature validation shape: \"+str(db.features_valid.shape)\n",
    "print \"labels validation shape: \"+str(db.labels_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even implement a '__call__' method which returns a training batch each time the object is called. \n",
    "\n",
    "What do we return each call? We keep track of the current example using the iteration_step and we shift with batch_size. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![generating_batches](img/batches.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what the call method returns at each interation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#what do we return at each call\n",
    "feat=db.features[db.iteration_step*db.batch_size:(db.iteration_step+1)*db.batch_size]\n",
    "feat=db.features[db.iteration_step*db.batch_size:(db.iteration_step+1)*db.batch_size,np.newaxis]\n",
    "lab=db.labels[db.iteration_step*db.batch_size:(db.iteration_step+1)*db.batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with CNN, we should add an axis for the channels: 3 (R,G,B) for image processing and in 1 in this case as the spectrogram can be regarded as a monochromatic image. Thus, we use np.newaxis to add a new axis. \n",
    "\n",
    "Different deep learning frameworks require different order for the axes. You can change it with np.swapaxes . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the first example, which is different every time because of the batch shuffling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features,labels = db()\n",
    "#we did  np.swapaxes for tensorflow in self.iterate() so we do it backwards\n",
    "features = np.swapaxes(features,1,3)\n",
    "\n",
    "print \"iteration step \"+str(db.iteration_step)\n",
    "print features.shape\n",
    "print labels.shape\n",
    "#we go back from categorical to numerical labels\n",
    "label = np.nonzero(labels[2,:])[0]\n",
    "print 'instrument: '+instruments[int(label)]\n",
    "\n",
    "plt.imshow(np.log10(1+100*features[2,0,:,:].T),interpolation='none', origin='lower')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you don't need the __call__ method and you want the object to return the whole data at the same time? \n",
    "\n",
    "We can do this with db.getData() and db.getValidation()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, labels, features_valid, labels_valid = db.getData()\n",
    "print \"training features shape \"+str(features.shape)\n",
    "print \"validation features shape \"+str(features_valid.shape)\n",
    "features_valid, labels_valid = db.getValidation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Datasets that don't fit into memory \n",
    "\n",
    "Make a list with all the files and the number of examples you can extract for each one, as we previously did.\n",
    "Then, load only a part of them in memory according to some memory limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory_limit=np.minimum(30,total_points)\n",
    "floatX=np.float32\n",
    "####features\n",
    "features=np.zeros((memory_limit,time_context,feature_size),dtype=floatX)\n",
    "####labels\n",
    "labels = np.zeros((memory_limit,1),dtype=floatX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parallel computation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
